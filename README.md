# NLP_final_project

DS1011-NLP Final Project Neural Machine Translation

1. Recurrent neural network based encoder-decoder without attention
   - vi-en: model_1_simple_vi_en.ipynb
   - zh-en: model_1_simple_zh_en.ipynb
2. Recurrent neural network based encoder-decoder with attention
   - dot_product based attention
    - vi-en: dotpro_atten_vi_en.ipynb
    - zh-en: dotpro_atten_zh_en.ipynb
   - neural net based attention
    - vi-en: neural_attent_vi_en.ipynb
    - zh-en: neural_attent_zh_en.ipynb
3. Replace the recurrent encoder with  convolutional  based encoder
   - Convolution based dot product attention: conv_dot_product_attention.ipynb
4. Transformer, fully self-attention translation system
   - Transformer.ipynb
5. Preprocessing:
   - Preprocessing.ipynb: for dot_product based attention model, Convolution based dot product attention model and Transformer
