{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import io\n",
    "import os\n",
    "import sacrebleu\n",
    "import math\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_idx = 0\n",
    "SOS_idx = 1\n",
    "EOS_idx = 2\n",
    "UNK_idx= 3\n",
    "batch_size = 64\n",
    "MAX_SENTENCE_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_pretrained_path = './wiki-news-300d-1M.vec'\n",
    "zh_pretrained_path = './cc.zh.300.vec'\n",
    "data_prefix = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Life in the deep oceans',\n",
       " 'With vibrant video clips captured by submarines , David Gallo takes us to some of Earth &apos;s darkest , most violent , toxic and beautiful habitats , the valleys and volcanic ridges of the oceans &apos; depths , where life is bizarre , resilient and shockingly abundant .',\n",
       " 'This is Bill Lange . I &apos;m Dave Gallo .',\n",
       " 'And we &apos;re going to tell you some stories from the sea here in video .',\n",
       " 'We &apos;ve got some of the most incredible video of Titanic that &apos;s ever been seen , and we &apos;re not going to show you any of it .',\n",
       " 'The truth of the matter is that the Titanic -- even though it &apos;s breaking all sorts of box office records -- it &apos;s not the most exciting story from the sea .',\n",
       " 'And the problem , I think , is that we take the ocean for granted .',\n",
       " 'When you think about it , the oceans are 75 percent of the planet .',\n",
       " 'Most of the planet is ocean water .',\n",
       " 'The average depth is about two miles .']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en = open('iwslt-zh-en/train.tok.en' , encoding='utf-8').read().strip().split('\\n')\n",
    "val_en = open('iwslt-zh-en/dev.tok.en', encoding='utf-8').read().strip().split('\\n')\n",
    "test_en = open('iwslt-zh-en/test.tok.en', encoding='utf-8').read().strip().split('\\n')\n",
    "train_en[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['深海 海中 的 生命   大卫   盖罗 ',\n",
       " '大卫   盖罗 通过 潜水 潜水艇 拍下 的 影片 把 我们 带到 了 地球 最 黑暗   最 险恶 同时 也 最美 美丽 的 生物 栖息 栖息地   这里 是 海洋 深处 的 峡谷 和 火山 山脊   这里 怪诞   适应 适应力 应力 强 而且 数量 惊人 的 生命  ',\n",
       " '大卫   盖罗   这位 是 比尔   兰格    我 是 大卫   盖罗  ',\n",
       " '我们 将 用 一些 影片 来讲 讲述 一些 深海 海里 的 故事  ',\n",
       " '我们 这 有 不少 精彩 的 泰坦 泰坦尼克 坦尼 尼克 的 影片    可惜 您 今天 看不到 不到  ',\n",
       " '泰坦 泰坦尼克 泰坦尼克号 坦尼 尼克 号   是 拿 了 不少 票房 冠军   但 事实 事实上 它 并 不是 关于 于海洋 海洋 的 最 刺激 的 故事  ',\n",
       " '原因 在于 我们 一直 没 把 海洋 当回事 回事 回事儿 事儿  ',\n",
       " '大家 想想   海洋 占 了 地球 球面 面积 的 75  ',\n",
       " '地球 的 大部 大部分 部分 都 是 海水  ',\n",
       " '海洋 的 平均 深度 是 两英里 英里 ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_zh = open('iwslt-zh-en/train.tok.zh', encoding='utf-8').read().strip().split('\\n')\n",
    "val_zh = open('iwslt-zh-en/dev.tok.zh', encoding='utf-8').read().strip().split('\\n')\n",
    "test_zh = open('iwslt-zh-en/test.tok.zh', encoding='utf-8').read().strip().split('\\n')\n",
    "train_zh[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data: 213376\n",
      "Length of val data: 1261\n",
      "Length of test data: 1397\n"
     ]
    }
   ],
   "source": [
    "print ('Length of train data:', len(train_en))\n",
    "print ('Length of val data:', len(val_en))\n",
    "print ('Length of test data:', len(test_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataframe(en, ch, len_raio=0.8):\n",
    "    en_len_list, ch_len_list = [], []\n",
    "    for en_sample, ch_sample in zip(en, ch):\n",
    "        en_len_list.append(len(en_sample))\n",
    "        ch_len_list.append(len(ch_sample))\n",
    "    df = pd.DataFrame({'en': en, \n",
    "                       'en_len': en_len_list,\n",
    "                       'ch': ch,\n",
    "                       'ch_len': ch_len_list\n",
    "                      })\n",
    "    en_len_at_ratio = sorted(en_len_list)[int(len_raio*len(en_len_list))]\n",
    "    ch_len_at_ratio = sorted(ch_len_list)[int(len_raio*len(ch_len_list))]\n",
    "    print (\"EN length @{}: {}, CH length @{}: {}\".format(len_raio, en_len_at_ratio, len_raio, ch_len_at_ratio))\n",
    "    return df, max(en_len_at_ratio, ch_len_at_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load pre-trained embedding and create the index2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabulary based on the training data\n",
    "from collections import Counter\n",
    "\n",
    "def get_vocabulary(data, VOCABULARY_SIZE=10000):\n",
    "    en_vocab, ch_vocab = [], []\n",
    "    for idx, row in data.iterrows():\n",
    "        en_vocab += row['en'].split()\n",
    "        ch_vocab += row['ch'].split()\n",
    "    en_token_counter = Counter(en_vocab)\n",
    "    ch_token_counter = Counter(ch_vocab)\n",
    "    print (\"Number of en words: {}, ch words: {}\".format(len(en_token_counter), len(ch_token_counter)))\n",
    "    en_word, _ = zip(*en_token_counter.most_common(VOCABULARY_SIZE))\n",
    "    en_id2token = ['<PAD>','<SOS>','<EOS>','<UNK>'] + list(en_word)\n",
    "    en_token2id = dict(zip(en_id2token, np.arange(len(en_id2token))))\n",
    "    ch_word, _ = zip(*ch_token_counter.most_common(VOCABULARY_SIZE))\n",
    "    ch_id2token = ['<PAD>','<SOS>','<EOS>','<UNK>'] + list(ch_word)\n",
    "    ch_token2id = dict(zip(ch_id2token, np.arange(len(ch_id2token))))\n",
    "    return en_id2token, en_token2id, ch_id2token, ch_token2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    \"\"\"\n",
    "    load the pretrained word embeddings\n",
    "    param fname: the path the to the word embedding\n",
    "    return: \n",
    "            a dictionary of the {word: embedding}\n",
    "    \"\"\"\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in tqdm(fin):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(data_path, callback, *callback_args):\n",
    "    \"\"\"\n",
    "    Create huge file with the callback function if not exist, otherwise load directly\n",
    "    param data_path: the path of the load file if exist, otherwise the path to store the created file\n",
    "    param callback: the function to generate the data if not exist\n",
    "    param callback_args: the argument for the callback:\n",
    "    return: \n",
    "            the data, either loaded or created by callback\n",
    "    \"\"\"\n",
    "\n",
    "    data_path = data_prefix+data_path\n",
    "    if os.path.isfile(data_path):\n",
    "        print ('File exists, load from path...')\n",
    "        data = pickle.load(open(data_path, 'rb'))\n",
    "    else:\n",
    "        print ('File not exists, creating...')\n",
    "        data = callback(*callback_args)\n",
    "        pickle.dump(data, open(data_path, 'wb'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_weights(whole_vec, id2token):\n",
    "    \"\"\"\n",
    "    get the embeddings based on the word, create the embedding matrix\n",
    "    param whole_vec: the dictionary of pretrained embeddings\n",
    "    param id2token: the whole vocabulary\n",
    "    return:\n",
    "            embedding matrix\n",
    "    \"\"\"\n",
    "    weight = np.zeros((len(id2token), len(whole_vec['sky'])))\n",
    "    mask = np.zeros((len(id2token)))\n",
    "    for i, word in enumerate(id2token[1:]):\n",
    "        if word in whole_vec.keys():\n",
    "            weight[i+1] = np.array(whole_vec[word])\n",
    "        else:\n",
    "            weight[i+1] = np.array(whole_vec['UNK'])\n",
    "            mask[i+1] = 1\n",
    "            print (\"Out of vocabulary word: \", word)   \n",
    "    return weight, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize english sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn a Unicode string to plain ASCII\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z]+\", r\" \", s)\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def normalizeZh(s):\n",
    "    #s = s.decode(\"utf8\")\n",
    "    #s = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）％]+\", \" \",s)\n",
    "    s = re.sub( '\\s+', ' ', s )\n",
    "    s = s.strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token to index function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_index(sentence, token2id):\n",
    "    indicies_data = []\n",
    "    for s in sentence:\n",
    "        tokens = s.split(' ')\n",
    "        index_list =[token2id[token] if token in token2id else UNK_idx for token in tokens]\n",
    "        assert len(tokens) == len(index_list)\n",
    "        indicies_data.append(index_list)\n",
    "    return indicies_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** English**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_train_en = [normalizeString(s) for s in train_en]\n",
    "normalize_val_en = [normalizeString(s) for s in val_en]\n",
    "normalize_test_en = [normalizeString(s) for s in test_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['life in the deep oceans',\n",
       " 'with vibrant video clips captured by submarines david gallo takes us to some of earth apos s darkest most violent toxic and beautiful habitats the valleys and volcanic ridges of the oceans apos depths where life is bizarre resilient and shockingly abundant',\n",
       " 'this is bill lange i apos m dave gallo',\n",
       " 'and we apos re going to tell you some stories from the sea here in video',\n",
       " 'we apos ve got some of the most incredible video of titanic that apos s ever been seen and we apos re not going to show you any of it',\n",
       " 'the truth of the matter is that the titanic even though it apos s breaking all sorts of box office records it apos s not the most exciting story from the sea',\n",
       " 'and the problem i think is that we take the ocean for granted',\n",
       " 'when you think about it the oceans are percent of the planet',\n",
       " 'most of the planet is ocean water',\n",
       " 'the average depth is about two miles']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_train_en[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Chinese**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_train_zh = [normalizeZh(s) for s in train_zh]\n",
    "normalize_val_zh = [normalizeZh(s) for s in val_zh]\n",
    "normalize_test_zh = [normalizeZh(s) for s in test_zh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['深海 海中 的 生命 大卫 盖罗',\n",
       " '大卫 盖罗 通过 潜水 潜水艇 拍下 的 影片 把 我们 带到 了 地球 最 黑暗 最 险恶 同时 也 最美 美丽 的 生物 栖息 栖息地 这里 是 海洋 深处 的 峡谷 和 火山 山脊 这里 怪诞 适应 适应力 应力 强 而且 数量 惊人 的 生命',\n",
       " '大卫 盖罗 这位 是 比尔 兰格 我 是 大卫 盖罗',\n",
       " '我们 将 用 一些 影片 来讲 讲述 一些 深海 海里 的 故事',\n",
       " '我们 这 有 不少 精彩 的 泰坦 泰坦尼克 坦尼 尼克 的 影片 可惜 您 今天 看不到 不到',\n",
       " '泰坦 泰坦尼克 泰坦尼克号 坦尼 尼克 号 是 拿 了 不少 票房 冠军 但 事实 事实上 它 并 不是 关于 于海洋 海洋 的 最 刺激 的 故事',\n",
       " '原因 在于 我们 一直 没 把 海洋 当回事 回事 回事儿 事儿',\n",
       " '大家 想想 海洋 占 了 地球 球面 面积 的 75',\n",
       " '地球 的 大部 大部分 部分 都 是 海水',\n",
       " '海洋 的 平均 深度 是 两英里 英里']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_train_zh[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN length @0.8: 139, CH length @0.8: 72\n",
      "EN length @0.8: 148, CH length @0.8: 80\n",
      "EN length @0.8: 123, CH length @0.8: 60\n"
     ]
    }
   ],
   "source": [
    "train_df, pad_len = to_dataframe(normalize_train_en, normalize_train_zh)\n",
    "val_df, _ = to_dataframe(normalize_val_en, normalize_val_zh)\n",
    "test_df, _ = to_dataframe(normalize_test_en, normalize_test_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of en words: 50902, ch words: 88914\n"
     ]
    }
   ],
   "source": [
    "en_id2token, en_token2id, zh_id2token, zh_token2id = get_vocabulary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10004\n"
     ]
    }
   ],
   "source": [
    "print(len(en_id2token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en_indicies = token_to_index(normalize_train_en, en_token2id)\n",
    "val_en_indicies = token_to_index(normalize_val_en, en_token2id)\n",
    "test_en_indicies = token_to_index(normalize_test_en, en_token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zh_indicies = token_to_index(normalize_train_zh, zh_token2id)\n",
    "val_zh_indicies = token_to_index(normalize_val_zh, zh_token2id)\n",
    "test_zh_indicies = token_to_index(normalize_test_zh, zh_token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[121, 12, 4, 669, 1720], [29, 7711, 426, 4951, 3109, 59, 3, 1095, 3, 536, 82, 7, 83, 8, 312, 6, 17, 9971, 122, 2356, 2679, 5, 428, 5494, 4, 8917, 5, 7173, 3, 8, 4, 1720, 6, 6262, 90, 121, 16, 3600, 6017, 5, 3, 5294], [18, 16, 1214, 3, 11, 6, 80, 4877, 3], [5, 15, 6, 41, 63, 7, 161, 14, 83, 441, 46, 4, 710, 71, 12, 426], [15, 6, 79, 116, 83, 8, 4, 122, 631, 426, 8, 7345, 10, 6, 17, 234, 101, 330, 5, 15, 6, 41, 38, 63, 7, 192, 14, 154, 8, 13]]\n",
      "[[3804, 3496, 4, 205, 1760, 3], [1760, 3, 124, 2374, 3, 4960, 4, 2102, 39, 7, 1693, 9, 295, 118, 2032, 118, 3, 198, 23, 4374, 1010, 4, 192, 3331, 4249, 102, 6, 611, 3406, 4, 5524, 14, 4280, 3, 102, 3, 1245, 3, 3, 1587, 128, 715, 1174, 4, 205], [1760, 3, 1189, 6, 2314, 3, 5, 6, 1760, 3], [7, 52, 84, 67, 2102, 2007, 835, 67, 3804, 3434, 4, 146], [7, 11, 15, 2591, 2269, 4, 3, 3, 3, 3519, 4, 2102, 5331, 983, 195, 1946, 473]]\n"
     ]
    }
   ],
   "source": [
    "print(train_en_indicies[:5])\n",
    "print(train_zh_indicies[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "File exists, load from path...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "File exists, load from path...\n"
     ]
    }
   ],
   "source": [
    "en_pretrain_emb, zh_pretrain_emb = [], []\n",
    "print ('-'*100)\n",
    "#en_pretrain_emb = load_pickle('en_pretrain_emb.p', load_vectors, en_pretrained_path)\n",
    "print ('-'*100)\n",
    "eng_embedding, _ = load_pickle('eng_embedding.p', get_embedding_weights, en_pretrain_emb, en_id2token)\n",
    "del en_pretrain_emb\n",
    "print ('-'*100)\n",
    "#zh_pretrain_emb = load_pickle('zh_pretrain_emb.p', load_vectors, zh_pretrained_path)\n",
    "print ('-'*100)\n",
    "zh_embedding, _ = load_pickle('zh_embedding.p', get_embedding_weights, zh_pretrain_emb, zh_id2token)\n",
    "del zh_pretrain_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10004, 300)\n"
     ]
    }
   ],
   "source": [
    "print(zh_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, source_lan, translate_lan):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.source_lan = source_lan\n",
    "        self.translate_lan = translate_lan\n",
    "        \n",
    "        assert (len(self.source_lan) == len(self.translate_lan))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_lan)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        source_lan_idx = self.source_lan[key][:MAX_SENTENCE_LENGTH-1]\n",
    "        translation_lan_idx = self.translate_lan[key][:MAX_SENTENCE_LENGTH-1]\n",
    "        source_lan_idx.append(EOS_idx)\n",
    "        translation_lan_idx.append(EOS_idx)\n",
    "        \n",
    "        return [source_lan_idx, translation_lan_idx, len(source_lan_idx), len(translation_lan_idx)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    source_list = []\n",
    "    translate_list = []\n",
    "    length_list = []\n",
    "    \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "\n",
    "        length_list.append(datum[2])\n",
    "        s_padded_vec = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=PAD_idx)\n",
    "        source_list.append(s_padded_vec)\n",
    "        t_padded_vec = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])),\n",
    "                                mode=\"constant\", constant_values=PAD_idx)\n",
    "        translate_list.append(t_padded_vec)\n",
    "        \n",
    "#     ind_dec_order = np.argsort(length_list)[::-1]\n",
    "#     source_list = np.array(source_list)[ind_dec_order]\n",
    "#     length_list = np.array(length_list)[ind_dec_order]\n",
    "#     translate_list = np.array(translate_list)[ind_dec_order]\n",
    "    \n",
    "    if torch.cuda.is_available and torch.has_cudnn:\n",
    "        return [torch.from_numpy(np.array(source_list)).cuda(),torch.from_numpy(np.array(translate_list)).cuda()]\n",
    "    else:\n",
    "        return [torch.from_numpy(np.array(source_list)),torch.from_numpy(np.array(translate_list))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LanguageDataset(train_zh_indicies, train_en_indicies)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "s_train_dataset = LanguageDataset(train_zh_indicies[2000:2500], train_en_indicies[2000:2500])\n",
    "s_train_loader = torch.utils.data.DataLoader(dataset=s_train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = LanguageDataset(val_zh_indicies, val_en_indicies)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_dataset = LanguageDataset(test_zh_indicies, test_en_indicies)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10004, 300)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rnn Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, emb_dim, hid_dim):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(zh_embedding), freeze=False)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        encode_batch_size, length = inputs.size()\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(inputs).float()) # the size -1 is inferred from other dimensions\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "        outputs = outputs[:, :, :self.hid_dim] + outputs[:, :, self.hid_dim:]\n",
    "        outputs = outputs.transpose(0,1)\n",
    "        hidden = hidden[0:1,:,:] + hidden[1:2,:,:]\n",
    "        #print(outputs.shape, hidden.shape)\n",
    "        return outputs, hidden #[T*B*H], [1*B*H]\n",
    "\n",
    "    def init_hidden(self, encode_batch_size):\n",
    "        \n",
    "        return torch.zeros(2, encode_batch_size, self.hid_dim, device=device)\n",
    "    \n",
    "class ConvEncoderRNN(nn.Module):\n",
    "    def __init__(self, emb_dim, hid_dim):\n",
    "        super(ConvEncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(zh_embedding), freeze=False)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.conv1 = nn.Conv1d(emb_dim, hid_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hid_dim, hid_dim, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(hid_dim, hid_dim, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv1d(hid_dim, hid_dim, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, seq_len1 = inputs.size()\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(inputs).float()) # B*T*EMD\n",
    "        hidden1 = self.conv1(embedded.transpose(1,2)).transpose(1,2)\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size, seq_len1, hidden1.size(-1))\n",
    "        hidden1 = self.conv2(hidden1.transpose(1,2)).transpose(1,2)\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size, seq_len1, hidden1.size(-1))\n",
    "        \n",
    "        hidden1 = self.conv3(hidden1.transpose(1,2)).transpose(1,2)\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size, seq_len1, hidden1.size(-1))\n",
    "        hidden1 = self.conv4(hidden1.transpose(1,2)).transpose(1,2)\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size, seq_len1, hidden1.size(-1))\n",
    "        \n",
    "        outputs = hidden1.transpose(0,1)\n",
    "        hidden = torch.sum(hidden1, dim=1).unsqueeze(0)\n",
    "        #print(outputs.shape, hidden.shape)\n",
    "        \n",
    "        return outputs, hidden #[T*B*H], [1*B*H]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder w/o Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):#[B*H], [T*B*H]\n",
    "        timestep = encoder_outputs.size(0)  #T\n",
    "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1) #[B*T*H]\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # [B*T*H]\n",
    "        attn_energies = self.score(h, encoder_outputs) #[B*T]\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1) #[B*1*T]\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        # [B*T*2H]->[B*T*H]\n",
    "        energy = F.relu(self.attn(torch.cat([hidden, encoder_outputs], 2)))\n",
    "        energy = energy.transpose(1, 2)  # [B*H*T]\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # [B*1*H]\n",
    "        energy = torch.bmm(v, energy)  # [B*1*T]\n",
    "        return energy.squeeze(1)  # [B*T]\n",
    "    \n",
    "class dotproduct_att(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(dotproduct_att, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn1 = nn.Linear(self.hidden_size, hidden_size)\n",
    "        self.attn2 = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):#[B*H], [T*B*H]\n",
    "        timestep = encoder_outputs.size(0)  #T\n",
    "        #h = hidden.repeat(timestep, 1, 1).transpose(0, 1) #[B*T*H]\n",
    "        encoder_outputs = self.attn1(encoder_outputs.transpose(0, 1)).transpose(1,2)  # [B*H*T]\n",
    "        attn_energies = torch.bmm(self.attn2(hidden).unsqueeze(1), encoder_outputs) #[B*T]\n",
    "        return F.softmax(attn_energies, dim=2) #[B*1*T]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, emb_dim, hid_dim, output_dim):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(eng_embedding), freeze=False)\n",
    "        self.mapping = nn.Linear(self.emb_dim+self.hid_dim, self.hid_dim)\n",
    "        #self.attention = Attention(hid_dim)\n",
    "        self.attention = Attention(hid_dim)\n",
    "        self.gru = nn.GRU(emb_dim+hid_dim, hid_dim)\n",
    "        \n",
    "        self.out = nn.Linear(hid_dim * 2, output_dim)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs, last_hidden, encoder_outputs): #encoder_outputs: [T*B*H]\n",
    "        \n",
    "        emb = self.dropout1(self.embedding(inputs).float()) #[1*B*EMB_DIM]\n",
    "        attn_keys = self.mapping(torch.cat((emb, last_hidden), dim=2))\n",
    "        attn_weights = self.attention(attn_keys[-1], encoder_outputs) #[[B*1*T]]\n",
    "        \n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # (B,1,H)\n",
    "        context = context.transpose(0, 1)  # (1,B,H)\n",
    "        \n",
    "        rnn_input = torch.cat([emb, context], 2) #[1*B*(EMD_DIM+H)]\n",
    "\n",
    "        output, hidden = self.gru(rnn_input, last_hidden) #[1*B*H], [1*B*H]\n",
    "        output = output.squeeze(0)  # (1,B,H) -> (B,H)\n",
    "        context = context.squeeze(0) # (1,B,H) -> [B*H]\n",
    "        output = self.out(torch.cat([output, context], 1)) #[B*2H]\n",
    "        output = self.softmax(output) #[B, output_dim]\n",
    "        \n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, embed_size, hidden_size, output_size,\n",
    "#                  n_layers=1, dropout=0.2):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.embed_size = embed_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.output_size = output_size\n",
    "#         self.n_layers = n_layers\n",
    "\n",
    "#         self.embed = nn.Embedding.from_pretrained(torch.from_numpy(eng_embedding), freeze=True)\n",
    "#         self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "#         self.attention = Attention(hidden_size)\n",
    "#         self.gru = nn.GRU(hidden_size + embed_size, hidden_size,\n",
    "#                           n_layers, dropout=dropout)\n",
    "#         self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "#     def forward(self, input, last_hidden, encoder_outputs):\n",
    "#         # Get the embedding of the current input word (last output word)\n",
    "#         embedded = self.embed(input)  # (1,B,N)\n",
    "#         embedded = self.dropout(embedded)\n",
    "#         # Calculate attention weights and apply to encoder outputs\n",
    "#         attn_weights = self.attention(last_hidden[-1], encoder_outputs)\n",
    "#         context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # (B,1,N)\n",
    "#         context = context.transpose(0, 1)  # (1,B,N)\n",
    "#         # Combine embedded input word and attended context, run through RNN\n",
    "#         rnn_input = torch.cat([embedded, context], 2)\n",
    "#         output, hidden = self.gru(rnn_input, last_hidden)\n",
    "#         output = output.squeeze(0)  # (1,B,N) -> (B,N)\n",
    "#         context = context.squeeze(0)\n",
    "#         output = self.out(torch.cat([output, context], 1))\n",
    "#         output = F.log_softmax(output, dim=1)\n",
    "#         return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, embed_size, hidden_size, n_layers=1, dropout=0.2):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.embed_size = embed_size\n",
    "#         self.embed = nn.Embedding.from_pretrained(torch.from_numpy(zh_embedding), freeze=False)\n",
    "#         self.gru = nn.GRU(embed_size, hidden_size, n_layers, dropout=dropout, bidirectional=True, batch_first=True)\n",
    "\n",
    "#     def forward(self, src, hidden=None):\n",
    "#         embedded = self.embed(src)\n",
    "#         outputs, hidden = self.gru(embedded, hidden)\n",
    "#         # sum bidirectional outputs\n",
    "#         outputs = (outputs[:, :, :self.hidden_size] +\n",
    "#                    outputs[:, :, self.hidden_size:])\n",
    "#         hidden = hidden = hidden[0:1,:,:]\n",
    "#         return outputs, hidden\n",
    "    \n",
    "#     def init_hidden(self, encode_batch_size):\n",
    "#         return torch.ones(2, encode_batch_size, self.hidden_size, device=device)\n",
    "\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, hidden_size):\n",
    "#         super(Attention, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "#         self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "#         stdv = 1. / math.sqrt(self.v.size(0))\n",
    "#         self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "#     def forward(self, hidden, encoder_outputs):\n",
    "#         timestep = encoder_outputs.size(0)\n",
    "#         h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
    "#         encoder_outputs = encoder_outputs.transpose(0, 1)  # [B*T*H]\n",
    "#         attn_energies = self.score(h, encoder_outputs)\n",
    "#         return F.relu(attn_energies, dim=1).unsqueeze(1)\n",
    "\n",
    "#     def score(self, hidden, encoder_outputs):\n",
    "#         # [B*T*2H]->[B*T*H]\n",
    "#         energy = F.softmax(self.attn(torch.cat([hidden, encoder_outputs], 2)))\n",
    "#         energy = energy.transpose(1, 2)  # [B*H*T]\n",
    "#         v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # [B*1*H]\n",
    "#         energy = torch.bmm(v, energy)  # [B*1*T]\n",
    "#         return energy.squeeze(1)  # [B*T]\n",
    "\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, embed_size, hidden_size, output_size,\n",
    "#                  n_layers=1, dropout=0.2):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.embed_size = embed_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.output_size = output_size\n",
    "#         self.n_layers = n_layers\n",
    "\n",
    "#         self.embed = nn.Embedding.from_pretrained(torch.from_numpy(eng_embedding), freeze=True)\n",
    "#         self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "#         self.attention = Attention(hidden_size)\n",
    "#         self.gru = nn.GRU(hidden_size + embed_size, hidden_size,\n",
    "#                           n_layers, dropout=dropout)\n",
    "#         self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "#     def forward(self, input, last_hidden, encoder_outputs):\n",
    "#         # Get the embedding of the current input word (last output word)\n",
    "#         embedded = self.embed(input)  # (1,B,N)\n",
    "#         embedded = self.dropout(embedded)\n",
    "#         # Calculate attention weights and apply to encoder outputs\n",
    "#         attn_weights = self.attention(last_hidden[-1], encoder_outputs)\n",
    "#         context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # (B,1,N)\n",
    "#         context = context.transpose(0, 1)  # (1,B,N)\n",
    "#         # Combine embedded input word and attended context, run through RNN\n",
    "#         rnn_input = torch.cat([embedded, context], 2)\n",
    "#         output, hidden = self.gru(rnn_input, last_hidden)\n",
    "#         output = output.squeeze(0)  # (1,B,N) -> (B,N)\n",
    "#         context = context.squeeze(0)\n",
    "#         output = self.out(torch.cat([output, context], 1))\n",
    "#         output = F.log_softmax(output, dim=1)\n",
    "#         return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.95\n",
    "#input_tensor: list of sentence tensor\n",
    "def train(source, translate, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "          criterion):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    cur_batch_size, input_length = source.size()\n",
    "    cur_batch_size, target_length = translate.size()\n",
    "    \n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden(cur_batch_size)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "    #hidden = [1, batch size, hid dim]\n",
    "    #encoder_output = [batch size, sen len, hid dim]\n",
    "    \n",
    "    encoder_output, encoder_hidden = encoder(source, encoder_hidden)\n",
    "    #encoder_output, encoder_hidden = encoder(source)\n",
    "\n",
    "    decoder_input = torch.tensor(np.array([[SOS_idx]]*cur_batch_size).reshape(1,cur_batch_size),device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for i in range(target_length):\n",
    "        \n",
    "            \n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "            \n",
    "            loss += criterion(decoder_output, translate[:,i])\n",
    "            decoder_input = translate[:,i].unsqueeze(0)  # Teacher forcing\n",
    "            \n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for i in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention= decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            \n",
    "            #decoder_input [1, batch size] \n",
    "            decoder_input = decoder_input.unsqueeze(0)\n",
    " \n",
    "            loss += criterion(decoder_output, translate[:,i])\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate2(encoder, decoder, loader):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    input_words = []\n",
    "    target_words = []\n",
    "    decoded_words = []\n",
    "    num_count = 0\n",
    "    num_count = 0\n",
    "    for i, (source, translate) in enumerate(loader):\n",
    "\n",
    "        cur_batch_size = translate.size()[0]\n",
    "        #print('cur_batch_size: ',cur_batch_size)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            encoder_hidden = encoder.init_hidden(cur_batch_size)\n",
    "        \n",
    "        input_tensor = source\n",
    "        target_tensor = translate\n",
    "        target_length = target_tensor.size()[1]\n",
    "        #print('encoder_hidden shape: ', encoder_hidden.size())\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "        #encoder_output, encoder_hidden = encoder(input_tensor)\n",
    "\n",
    "        decoder_input = torch.tensor(np.array([[SOS_idx]]*cur_batch_size).reshape(1,cur_batch_size),device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        for i in range(cur_batch_size):\n",
    "            decoded_words.append([])\n",
    "            input_words.append([])\n",
    "            target_words.append([])\n",
    "        \n",
    "        for i in range(cur_batch_size):\n",
    "            for ii in range(input_tensor.size()[1]):\n",
    "                if zh_id2token[input_tensor.cpu().numpy()[i,ii]] != '<PAD>':\n",
    "                    input_words[num_count].append(zh_id2token[input_tensor.cpu().numpy()[i,ii]])\n",
    "            num_count += 1\n",
    "        num_count -= cur_batch_size\n",
    "        for i in range(cur_batch_size):\n",
    "            for ii in range(target_tensor.size()[1]):\n",
    "                if en_id2token[target_tensor.cpu().numpy()[i,ii]] != '<PAD>':\n",
    "                    target_words[num_count].append(en_id2token[target_tensor.cpu().numpy()[i,ii]])\n",
    "            num_count += 1\n",
    "        num_count -= cur_batch_size      \n",
    "        \n",
    "        cur_len = np.zeros(cur_batch_size, dtype=int)\n",
    "        #pdb.set_trace()\n",
    "        for i in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            decoder_input = decoder_input.unsqueeze(0)\n",
    "            \n",
    "            topi = topi.squeeze().cpu().numpy()\n",
    "            if cur_len[0] == 0:\n",
    "                for i in range(len(topi)):\n",
    "                    decoded_words[num_count+i].append(en_id2token[topi[i]])\n",
    "                    cur_len[i] += 1\n",
    "            \n",
    "            else:\n",
    "                for i in range(len(topi)):\n",
    "                    if decoded_words[num_count+i][cur_len[i]-1] == '<EOS>':\n",
    "                        continue\n",
    "                    decoded_words[num_count+i].append(en_id2token[topi[i]])\n",
    "                    cur_len[i] += 1\n",
    "        num_count += cur_batch_size\n",
    "    pre_list = []\n",
    "    for pre_sentenc in decoded_words:\n",
    "        pre_list.append(\"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in pre_sentenc]).strip())\n",
    "\n",
    "    true_list = []\n",
    "    for true_sentenc in target_words:\n",
    "        true_list.append(\"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in true_sentenc]).strip())\n",
    "    \n",
    "    true_list2 = []\n",
    "    true_list2.append(true_list)        \n",
    "    score = sacrebleu.corpus_bleu(pre_list, true_list2)\n",
    "    #score = sacrebleu.raw_corpus_bleu(pre_list, true_list2)\n",
    "    print('bleu score: ', score.score)\n",
    "    return decoded_words,input_words, target_words, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=200, plot_every=200, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss(ignore_index = 0)\n",
    "    count_iter = 0\n",
    "    for cur_iter in range(1, n_iters + 1):\n",
    "        for i, (source, translate) in enumerate(train_loader):\n",
    "            \n",
    "            loss = train(source, translate, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "            count_iter += 1\n",
    "            if count_iter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%d %d%% %.4f' % (cur_iter, cur_iter / n_iters * 100, print_loss_avg))\n",
    "                print('validation: ')\n",
    "                _,_,_, score = evaluate2(encoder, decoder, val_loader)\n",
    "                with open('out5.txt', 'a') as file:\n",
    "                    file.write('validation: ' + str(score.score) + '\\n')\n",
    "                print('small train: ')\n",
    "                _,_,_, score = evaluate2(encoder, decoder, s_train_loader)\n",
    "                with open('out5.txt', 'a') as file:\n",
    "                    file.write('small train: ' + str(score.score) + '\\n')\n",
    "\n",
    "            if count_iter % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "    \n",
    "    \n",
    "    showPlot(plot_losses)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "#%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()\n",
    "    \n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENC_EMB_DIM = 300\n",
    "ZH_EMB_DIM = 300\n",
    "HID_DIM = 300\n",
    "OUTPUT_DIM = len(zh_id2token)\n",
    "\n",
    "\n",
    "#encoder = ConvEncoderRNN(ENC_EMB_DIM, HID_DIM).to(device)\n",
    "encoder = EncoderRNN(ENC_EMB_DIM, HID_DIM).to(device)\n",
    "decoder = DecoderRNN(ZH_EMB_DIM, HID_DIM, OUTPUT_DIM).to(device)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainIters(encoder, decoder, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(encoder.state_dict(), './model/encoder_stable_ATT_ming.pth')\n",
    "# torch.save(decoder.state_dict(), './model/decoder_stable_ATT_ming.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load('./model/encoder_stable_ATT_ming.pth'))\n",
    "decoder.load_state_dict(torch.load('./model/decoder_stable_ATT_ming.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "他 还 上传 了 许多 自然 自然景观 景观 的 视频\n"
     ]
    }
   ],
   "source": [
    "source = normalizeZh('他 还 上传 了 许多 自然 自然景观 景观 的 视频')\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager\n",
    "\n",
    "fontP = font_manager.FontProperties(fname='/home/zq415/msyh.ttf')\n",
    "# fontP.set_family('SimHei')\n",
    "\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.font_manager as fm\n",
    "# font = fm.FontProperties(fname='c:\\\\windows\\\\fonts\\\\simsun.ttc')  # speicify font\n",
    "# ax = most_active_posts.plot(x = 'title',y = 'active_span',kind = 'barh')\n",
    "# ax.set_xticklabels(most_active_posts['title'].str.decode('utf-8'), fontproperties=font)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    max_len = max(len(input_sentence.split(' ')), len(output_words))\n",
    "    attentions = attentions[:max_len+1, :max_len+1]\n",
    "    plt.matshow(attentions.numpy())\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes .encode('utf-8').decode(\n",
    "    ax.set_xticklabels([''] + [i for i in input_sentence.split(' ')] +\n",
    "                       ['<EOS>'], rotation=90,fontproperties=fontP,fontsize=10)\n",
    "    ax.set_yticklabels([''] + output_words,fontsize=10)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_visulization(encoder, decoder, source = u'他 还 上传 了 许多 自然 自然景观 景观 的 视频'):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    norm_source = [normalizeZh(source)]\n",
    "    source_idx = token_to_index(norm_source, zh_token2id)[0]\n",
    "    source_idx.append(EOS_idx)\n",
    "    source_idx_pad = np.pad(np.array(source_idx),\n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-len(source_idx))),\n",
    "                                mode=\"constant\", constant_values=PAD_idx)\n",
    "    \n",
    "    source_idx_pad_tensor = torch.from_numpy(np.array(source_idx_pad)).unsqueeze(0).cuda()\n",
    "    \n",
    "\n",
    "    cur_batch_size = 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_hidden = encoder.init_hidden(cur_batch_size)\n",
    "\n",
    "    input_tensor = source_idx_pad_tensor\n",
    "    \n",
    "    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "\n",
    "    \n",
    "    decoder_input = torch.tensor(np.array([[SOS_idx]]*cur_batch_size).reshape(1,cur_batch_size),device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    decoded_att = []\n",
    "    decoder_attentions = torch.zeros(MAX_SENTENCE_LENGTH, MAX_SENTENCE_LENGTH)\n",
    "    cur_len = 0\n",
    "    for i in range(MAX_SENTENCE_LENGTH):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "        decoder_attentions[cur_len] = decoder_attention.data\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "        decoder_input = decoder_input.unsqueeze(0).reshape(1,cur_batch_size)\n",
    "        \n",
    "        topi = topi.squeeze().cpu().numpy()\n",
    "        if cur_len == 0:\n",
    "            decoded_words.append(en_id2token[topi])\n",
    "            cur_len += 1\n",
    "        else:\n",
    "            if decoded_words[cur_len-1] == '<EOS>':\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(en_id2token[topi])\n",
    "                cur_len += 1\n",
    "                \n",
    "    return decoded_words, source, decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_words, source, decoder_attentions = evaluate_visulization(encoder, decoder)#, source = u'在 自创 流行 行文 文化 中 我们 都 有 自主 自主权 主权')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b8ec1b10588>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b8ec1b35160>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "showAttention(source, decoded_words, decoder_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder.load_state_dict(torch.load('./model/encoder_3.pth'))\n",
    "# decoder.load_state_dict(torch.load('./model/encoder_3.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset2 = LanguageDataset(val_zh_indicies, val_en_indicies)\n",
    "val_loader2 = torch.utils.data.DataLoader(dataset=val_dataset2,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "# decoded_words = evaluate_beam(encoder, decoder, val_loader2, beam_size=5, max_length=MAX_SENTENCE_LENGTH)\n",
    "# output_words, input_words, target_words, score= evaluate2(encoder, decoder, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_beam_size in range(2,5):\n",
    "    print(i_beam_size)\n",
    "    decoded_words = evaluate_beam(encoder, decoder, val_loader2, beam_size=i_beam_size, max_length=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score:  17.45114372890806\n",
      "bleu score:  18.105183772823043\n"
     ]
    }
   ],
   "source": [
    "output_words, input_words, target_words, score= evaluate2(encoder, decoder, val_loader)\n",
    "#output_words, input_words, target_words, score= evaluate2(encoder, decoder, small_train_loader)\n",
    "output_words, input_words, target_words, score= evaluate2(encoder, decoder, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(output_words)):\n",
    "    print('input: ', input_words[i])\n",
    "    print('target: ', target_words[i])\n",
    "    print('predict: ', output_words[i])\n",
    "    print('-----------------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self, decoder_hidden, last_idx=SOS_idx, sentence_idxes=[], sentence_scores=[]):\n",
    "        if(len(sentence_idxes) != len(sentence_scores)):\n",
    "            raise ValueError(\"length of indexes and scores should be the same\")\n",
    "        self.decoder_hidden = decoder_hidden\n",
    "        self.last_idx = last_idx\n",
    "        self.sentence_idxes =  sentence_idxes\n",
    "        self.sentence_scores = sentence_scores\n",
    "\n",
    "    def avgScore(self):\n",
    "        if len(self.sentence_scores) == 0:\n",
    "            return -100000\n",
    "            #raise ValueError(\"Calculate average score of sentence, but got no word\")\n",
    "        # return mean of sentence_score\n",
    "        return sum(self.sentence_scores) / len(self.sentence_scores)\n",
    "\n",
    "    def addTopk(self, topi, topv, decoder_hidden, beam_size):\n",
    "        #topv = torch.log(topv)\n",
    "        terminates, sentences = [], []\n",
    "        for i in range(beam_size):\n",
    "            if topi[0][i] == EOS_idx:\n",
    "                terminates.append(([en_id2token[idx.item()] for idx in self.sentence_idxes] + ['<EOS>'],\n",
    "                                   self.avgScore())) # tuple(word_list, score_float\n",
    "                continue\n",
    "            idxes = self.sentence_idxes[:] # pass by value\n",
    "            scores = self.sentence_scores[:] # pass by value\n",
    "            idxes.append(topi[0][i])\n",
    "            scores.append(topv[0][i])\n",
    "            sentences.append(Sentence(decoder_hidden, topi[0][i], idxes, scores))\n",
    "        return terminates, sentences\n",
    "\n",
    "    def toWordScore(self):\n",
    "        words = []\n",
    "        for i in range(len(self.sentence_idxes)):\n",
    "            if self.sentence_idxes[i] == EOS_idx:\n",
    "                words.append('<EOS>')\n",
    "            else:\n",
    "                words.append(en_id2token[self.sentence_idxes[i].item()])\n",
    "        if self.sentence_idxes[-1] != EOS_idx:\n",
    "            words.append('<EOS>')\n",
    "        return (words, self.avgScore())\n",
    "\n",
    "def beam_decode(decoder, decoder_hidden, encoder_outputs, beam_size, cur_batch_size, max_length=MAX_SENTENCE_LENGTH):\n",
    "    terminal_sentences, prev_top_sentences, next_top_sentences = [], [], []\n",
    "    prev_top_sentences.append(Sentence(decoder_hidden))\n",
    "    for i in range(max_length):\n",
    "        for sentence in prev_top_sentences:\n",
    "            decoder_input = torch.tensor(np.array([[sentence.last_idx]]*cur_batch_size).reshape(1,cur_batch_size),device=device)\n",
    "            #decoder_input = torch.LongTensor([[sentence.last_idx]])\n",
    "            #decoder_input = decoder_input.to(device)\n",
    "            decoder_hidden = sentence.decoder_hidden\n",
    "            decoder_output, decoder_hidden, _ = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            topv, topi = decoder_output.topk(beam_size)\n",
    "            term, top = sentence.addTopk(topi, topv, decoder_hidden, beam_size)\n",
    "            terminal_sentences.extend(term)\n",
    "            next_top_sentences.extend(top)\n",
    "\n",
    "        next_top_sentences.sort(key=lambda s: s.avgScore(), reverse=True)\n",
    "        prev_top_sentences = next_top_sentences[:beam_size]\n",
    "        next_top_sentences = []\n",
    "\n",
    "    terminal_sentences += [sentence.toWordScore() for sentence in prev_top_sentences]\n",
    "    terminal_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "    n = 1\n",
    "    return terminal_sentences[:1]\n",
    "\n",
    "def evaluate_beam(encoder, decoder, loader, beam_size, max_length=MAX_SENTENCE_LENGTH):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    input_words = []\n",
    "    target_words = []\n",
    "    decoded_words = []\n",
    "    num_count = 0\n",
    "    for i, (source, translate) in enumerate(loader):\n",
    "        cur_batch_size = translate.size()[0]\n",
    "        with torch.no_grad():\n",
    "            encoder_hidden = encoder.init_hidden(cur_batch_size)\n",
    "            \n",
    "        for i in range(cur_batch_size):\n",
    "            input_words.append([])\n",
    "            target_words.append([])\n",
    "        \n",
    "        for i in range(cur_batch_size):\n",
    "            for ii in range(source.size()[1]):\n",
    "                if zh_id2token[source.cpu().numpy()[i,ii]] != '<PAD>':\n",
    "                    input_words[num_count].append(zh_id2token[source.cpu().numpy()[i,ii]])\n",
    "            num_count += 1\n",
    "        num_count -= cur_batch_size\n",
    "        for i in range(cur_batch_size):\n",
    "            for ii in range(translate.size()[1]):\n",
    "                if en_id2token[translate.cpu().numpy()[i,ii]] != '<PAD>':\n",
    "                    target_words[num_count].append(en_id2token[translate.cpu().numpy()[i,ii]])\n",
    "            num_count += 1\n",
    "        \n",
    "        encoder_outputs, encoder_hidden = encoder(source, encoder_hidden)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoded_words.append(beam_decode(decoder, decoder_hidden, encoder_outputs, beam_size, cur_batch_size))\n",
    "        \n",
    "    pre_list = []\n",
    "    for i in range(len(decoded_words)):\n",
    "        pre_list.append(\"\".join([\" \"+ i if not i.startswith(\"'\") and i not in string.punctuation else i for i in decoded_words[i][0][0]]).strip())\n",
    "    \n",
    "    true_list = []\n",
    "    for true_sentenc in target_words:\n",
    "        true_list.append(\"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in true_sentenc]).strip())\n",
    "    \n",
    "    score = sacrebleu.corpus_bleu(pre_list, [true_list])\n",
    "    print('bleu score: ', score.score)\n",
    "    \n",
    "    return decoded_words, target_words, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score:  18.49002961202398\n"
     ]
    }
   ],
   "source": [
    "test_dataset2 = LanguageDataset(test_zh_indicies, test_en_indicies)\n",
    "test_loader2 = torch.utils.data.DataLoader(dataset=test_dataset2,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "decoded_words = evaluate_beam(encoder, decoder, test_loader2, beam_size=2, max_length=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(output_words)):\n",
    "    print('input: ', input_words[i])\n",
    "    print('target: ', target_words[i])\n",
    "    print('predict: ', output_words[i])\n",
    "    print('-----------------------------------------------')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
